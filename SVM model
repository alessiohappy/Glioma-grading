#Load the libraries
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import OneHotEncoder
import numpy as np
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
from sklearn.svm import SVC
from scikitplot.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt
from bayes_opt import BayesianOptimization
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import PrecisionRecallDisplay
from sklearn.calibration import CalibrationDisplay


'''1: Pre-processing'''
#Import dataset
df = pd.read_csv("C:\\Users\\aless\\OneDrive - University of Pisa\\"
                 + "ML playground\\Playground_tabular data\\Glioma grading\\"
                 + "glioma.csv", sep = ",", decimal = ".")
df = df.dropna()

c = df.columns

df = df.drop(["glioma", "Sex", "Ethnicity"], axis = 1)

#Define class, continous and categorical variables 
y = df['Grade']

df_no_Pheno = df.drop('Grade', axis = 1)

cont_var = ["Age_at_diagnosis"]
    
cat_var = ["Race"]

other_var = ['Gender', 'IDH1', 'TP53', 'ATRX', 'PTEN', 'EGFR', 'CIC', 'MUC16',
             'PIK3CA', 'NF1', 'PIK3R1', 'FUBP1', 'RB1', 'NOTCH1', 'BCOR',
             'CSMD3', 'SMARCA4', 'GRIN2A', 'IDH2', 'FAT4', 'PDGFRA']
    
df_cat_var = df_no_Pheno.drop(cont_var, axis = 1)
df_cat_var = df_cat_var.drop(other_var, axis = 1)

df_cont_var = df_no_Pheno.drop(cat_var, axis = 1)
df_cont_var = df_cont_var.drop(other_var, axis = 1)

df_other_var = df_no_Pheno.drop(cat_var, axis = 1)
df_other_var = df_other_var.drop(cont_var, axis = 1)

# Preprocessing: dummy variables 
enc = OneHotEncoder()
enc.fit(df_cat_var)
cat_var_encoded = enc.transform(df_cat_var).toarray()

cat_var_name = df_cat_var.columns
new_cat_var_names = enc.get_feature_names_out(cat_var_name)
new_df_cat_var = pd.DataFrame(cat_var_encoded, columns = new_cat_var_names)


# Create a df with normalize variables and cat variables 
new_df = np.concatenate((df_cont_var, df_other_var), axis=1)
new_df_1 = np.concatenate((new_df, new_df_cat_var), axis=1)
new_col_names = ["Age_at_diagnosis", 'Gender', 'IDH1', 'TP53', 'ATRX', 'PTEN',
                 'EGFR', 'CIC', 'MUC16', 'PIK3CA', 'NF1', 'PIK3R1', 'FUBP1',
                 'RB1', 'NOTCH1', 'BCOR', 'CSMD3', 'SMARCA4', 'GRIN2A', 'IDH2',
                 'FAT4', 'PDGFRA', "Race_0.0", "Race_1.0", "Race_2.0"]

X = pd.DataFrame(data=new_df_1, columns=new_col_names)


'''2: Bayesian Optimization'''
# Undersampling dataset 
rus = RandomUnderSampler(sampling_strategy = 0.80)
X_res, y_res = rus.fit_resample(X, y)

print(sorted(Counter(y_res).items()))

# Classification task
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.2, stratify = y_res)


'''Bayesian Optimization'''
def opt_svc(C, gamma):
    params = {
        'C': float(C),
        'gamma' : float(gamma)
        }  
# Execute the cross-validation phase with the given paramters
    model = SVC(**params,
                kernel = "rbf",
                class_weight = "balanced",
                probability = True)
    scores = cross_val_score(model, X_train, y_train, cv = 3, scoring = "recall")
    return scores.mean()

# Define the parameter space for optimization
pbounds = {'C': (0.001, 10),
           'gamma' : (0.001, 0.1)}

# Create the bayesian optimization object
bayopt = BayesianOptimization(f = opt_svc,
                              pbounds = pbounds)

# Bayesian Optimization
bayopt.maximize(init_points = 50, n_iter = 100)
print("Best hyperparameters:", bayopt.max)
best_params = bayopt.max['params']


'''3: ML model'''
# Create a SVC model
svc = SVC(C = float(best_params['C']),
          gamma = float(best_params['gamma']),
          kernel = "rbf",
          class_weight = "balanced",
          probability = True)

# Fit the SVC object to the data
svc.fit(X_train, y_train)

# Test the model
y_pred = svc.predict(X_test)


'''4: Model evaluation'''
# accuracy
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)

# Compute the classification report
report = classification_report(y_test, y_pred)
print(classification_report(y_test, y_pred))

# Learning curve
train_sizes, train_scores, test_scores = learning_curve(svc, X_train, y_train, cv = 3, scoring = 'recall')
train_scores_mean = np.mean(train_scores, axis = 1)
train_scores_std = np.std(train_scores, axis = 1)
test_scores_mean = np.mean(test_scores, axis = 1)
test_scores_std = np.std(test_scores, axis = 1)
# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.title("Learning curve")
plt.xlabel("data dimension")
plt.ylabel("Recall")
plt.grid()
plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha = 0.1, color = "r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha = 0.1, color = "g")
plt.plot(train_sizes, train_scores_mean, 'o-', color = "r", label = "Training Recall")
plt.plot(train_sizes, test_scores_mean, 'o-', color = "g", label = "Validation Recall")
plt.legend(loc = "best")
plt.show()

'''# Loss Function'''
# Calculate accuracy on training and validation sets
train_acc = accuracy_score(y_train, svc.predict(X_train))
val_acc = accuracy_score(y_test, svc.predict(X_test))
print(f'Training accuracy: {train_acc}')
print(f'Validation accuracy: {val_acc}')

# Plotting the training and validation accuracy
plt.plot([1], [train_acc], 'bo', label='Training accuracy')
plt.plot([1], [val_acc], 'ro', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Confusion matrix
confusion_matrix(y_test, y_pred)
plot_confusion_matrix(y_test, y_pred)

# Confusion matrix
confusion_matrix(y_test, y_pred)
plot_confusion_matrix(y_test, y_pred, normalize = True)

'''# AUC and PR Curves'''
# Predict probabilities
y_scores = svc.decision_function(X_test)
fpr, tpr, _ = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color = 'darkorange', lw = 2, label = 'ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color = 'navy', lw = 2, linestyle = '--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot PR Curve
plt.rcParams['figure.dpi'] = 600
display = PrecisionRecallDisplay.from_predictions(y_test, y_scores, name = "SVMClassifier")
_ = display.ax_.set_title("Precision-Recall curve")

# calibration plot
# calibration from estimation
disp = CalibrationDisplay.from_estimator(svc, X_test, y_test)


'''#3: SHapely Additive exPlanations'''
# Creazione del modello SHAP Explainer
final_shap_df_v2 = pd.DataFrame(data = X_res, columns = new_col_names)
shap_values_tree = shap.KernelExplainer(svc.predict, data = X_train).shap_values(X = final_shap_df_v2, y = y_res)
print(shap_values_tree)

# Summary_plot and dependence plot
shap.summary_plot(shap_values_tree, final_shap_df_v2)
shap.summary_plot(shap_values_tree, final_shap_df_v2, plot_type = "bar")

# Split the dataset and shap value in cont and cat variables and keep only cat
X_cat_var = X_res.drop(cont_var, axis = 1)
X_cat_var = X_cat_var.drop(other_var, axis = 1)

shap_values_tree_name = pd.DataFrame(shap_values_tree_cases, columns = new_col_names)

shap_values_categorical = shap_values_tree_name.drop(cont_var, axis = 1)
shap_values_categorical = shap_values_categorical.drop(other_var, axis = 1)

# Get number of unique categories for each feature 
print(new_df_cat_var.columns)
feature_names = X_cat_var.columns
n_categories = [3]

new_shap_values = []
for values in shap_values_categorical.values:
    values_split = np.split(values , np.cumsum(n_categories)) #split shap values into a list for each feature
    values_sum = [sum(l) for l in values_split] #sum values within each list
    new_shap_values.append(values_sum)

# Replace shap values for categorical variables
new_shap_values_categorical = np.array(new_shap_values)

categorical_variables_name = ["Race", "Null"]

new_shap_values_categorical_name = pd.DataFrame(new_shap_values_categorical, columns = categorical_variables_name)

categorical_shap_values_name = new_shap_values_categorical_name.drop('Null', axis = 1)

# Concatenate shap values for categorial and continue variables
continue_shap_value = shap_values_tree_name.drop(feature_names, axis = 1)

Final_shap_value = np.concatenate((continue_shap_value, categorical_shap_values_name), axis = 1)
columns_for_final_shap_value = ["Age_at_diagnosis", "Race", 'Gender', 'IDH1','TP53', 'ATRX', 'PTEN', 'EGFR', 'CIC', 'MUC16','PIK3CA', 'NF1', 'PIK3R1', 'FUBP1', 'RB1', 'NOTCH1', 'BCOR','CSMD3', 'SMARCA4', 'GRIN2A', 'IDH2', 'FAT4', 'PDGFRA']

Final_shap_values_name = pd.DataFrame(Final_shap_value, columns = columns_for_final_shap_value)

rus_2 = RandomUnderSampler(sampling_strategy = 0.80)
X_res_2, y_res_2 = rus.fit_resample(df_no_Pheno, y)

# Convert pandas df to np 
prova_1 = Final_shap_values_name.to_numpy()
prova_2 = X_res_2.to_numpy()
final_shap_values_tree = pd.DataFrame(prova_1, columns = columns_for_final_shap_value)
final_db_plot = pd.DataFrame(prova_2, columns = columns_for_final_shap_value)

# Summary_plot and dependence plot
fig = plt.subplots(dpi = 1000)
plt.title("SVM", y = 1.02, x = 0.5)
shap.summary_plot(prova_1, prova_2, feature_names = columns_for_final_shap_value)
fig = plt.subplots(dpi = 1000)
shap.summary_plot(prova_1, prova_2, feature_names = columns_for_final_shap_value, plot_type = "bar")
